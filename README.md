# Hello-Tensorflow

## 目录

```bash
.
├───Course
│   ├───1.0
│   │   ├───Basic
│   │   └───Example
│   └───2.0
│       ├───Basic
│       └───Example
└───Officials
    ├───Basic
    └───Pro
```

## 梯度下降法总结

+ 随机梯度下降（SGD）每次迭代只取一条样本数据，由于单个样本的训练可能会带来很多噪声，使得SGD并不是每次迭代都向着整体最优方向，因此在刚开始训练时可能收敛得很快，但是训练一段时间后就会变得很慢。
+ 批量梯度下降（BGD）每次迭代都考虑全部样本，做的是全局优化，但花费的计算资源较大，如果训练数据非常大，还无法实现全部样本同步参与。

> 在SGD和BGD中，还有一个集合了两种梯度下降法优点的方法：小批量梯度下降法（MBGD，Mini-batch gradient descent），每次迭代从训练样本中随机抽取一小批进行训练，这个一小批的数量取值也是一个超参数。